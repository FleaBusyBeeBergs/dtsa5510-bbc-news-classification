{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8049e335-f43a-4680-8b5f-e11f9301a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #\n",
    "import numpy as np #\n",
    "import re #\n",
    "import nltk #\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt #\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer #\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #\n",
    "from sklearn import decomposition #\n",
    "# from sklearn.decomposition import TruncatedSVD, NMF\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.metrics import silhouette_score, accuracy_score\n",
    "# from sklearn.preprocessing import normalize\n",
    "# from scipy.sparse import SparseEfficiencyWarning\n",
    "from sklearn.model_selection import train_test_split #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a10c08-6b54-40c7-9663-a667a95f1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bergs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884f24b6-51fc-45f0-9509-d133690c48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/BBC News Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24891e6c-1d6b-4b76-a773-21271669320b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713116b1-5a6e-4d2c-9a45-055386ba896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc9fb4f-4341-4506-95c3-853d92b45990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "sport            346\n",
       "business         336\n",
       "politics         274\n",
       "entertainment    273\n",
       "tech             261\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b3bba-6317-4742-aedd-945ef322ebb9",
   "metadata": {},
   "source": [
    "complaints -> Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff5292-7004-4bee-9fee-1c009b022d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove duplicates\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # number of duplicates\n",
    "    number_duplicates = df['Text'].duplicated().sum()\n",
    "\n",
    "    # print no. duplicates\n",
    "    print(f'No. of duplicate articles: {number_duplicates}' '\\n')\n",
    "\n",
    "    # removal\n",
    "    df_cleaned = df.drop_duplicates(subset = ['Text'])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# function to remove non-alphabetical characters\n",
    "def remove_non_alpha(text):\n",
    "    \n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5918dc-ec88-4748-9f01-d16523fd1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab749fdb-c4a9-45e2-a0d0-3a7558b30f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove duplicates (run this before tokenization)\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # number of duplicates\n",
    "    number_duplicates = df['Text'].duplicated().sum()\n",
    "\n",
    "    # print no. duplicates\n",
    "    print(f'No. of duplicate articles: {number_duplicates}' '\\n')\n",
    "\n",
    "    # removal\n",
    "    df_cleaned = df.drop_duplicates(subset = ['Text'])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# function to remove non-alphabetical characters (integrated into tokenize function)\n",
    "def remove_non_alpha(text):\n",
    "    \n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "# function for cleaning, rather than using defaults\n",
    "def tokenize(text):\n",
    "    \n",
    "    text = remove_non_alpha(text)\n",
    "    \n",
    "    tokens = [word for word in nltk.word_tokenize(text) if (len(word)) > 3]\n",
    "    tokens = map(str.lower, tokens)\n",
    "    stems = [stemmer.stem(item) for item in tokens if (item not in stop_words)]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a9891a0-fc2f-46b0-aae1-1563910a21ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of duplicate articles: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = remove_duplicates(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a6038a1-ea97-4684-bbc3-297d3f494387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1440 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1440 non-null   int64 \n",
      " 1   Text       1440 non-null   object\n",
      " 2   Category   1440 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 45.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f951f844-acfe-4089-862e-427fdf122839",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = (TfidfVectorizer(tokenizer = tokenize, \n",
    "                              stop_words = None,\n",
    "                              max_df = 0.75, # max freq of word in documents\n",
    "                              max_features = 1000,\n",
    "                              lowercase = False,\n",
    "                              ngram_range = (1, 2),\n",
    "                              token_pattern = None) # turn off token warning \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a9391d-394b-4163-9a8a-3d0c96d65dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(train_df.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd63b17c-1bcf-4ca8-8f33-9c9a3f787522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.04267657, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dense = tfidf_vectors.toarray()\n",
    "tfidf_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "804d3171-43b8-4d62-ab01-9fc999f90ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abil', 'abl', 'academi', 'accept', 'access', 'accord', 'account',\n",
       "       'accus', 'achiev', 'across', 'act', 'action', 'activ', 'actor',\n",
       "       'actress', 'actual', 'ad', 'address', 'admit', 'affair', 'affect',\n",
       "       'africa', 'agenc', 'agre', 'agreement', 'ahead', 'aim', 'airlin',\n",
       "       'alan', 'album', 'alleg', 'allow', 'almost', 'along', 'alreadi',\n",
       "       'also', 'although', 'alway', 'america', 'american', 'among',\n",
       "       'amount', 'analyst', 'andi', 'andrew', 'announc', 'annual',\n",
       "       'anoth', 'answer', 'anyth', 'appeal', 'appear', 'appl', 'approach',\n",
       "       'approv', 'april', 'area', 'argu', 'around', 'arrest', 'arsenal',\n",
       "       'artist', 'ask', 'associ', 'asylum', 'athlet', 'attack', 'attempt',\n",
       "       'attend', 'attract', 'audienc', 'august', 'australia',\n",
       "       'australian', 'author', 'avail', 'averag', 'aviat', 'avoid',\n",
       "       'award', 'away', 'back', 'ball', 'band', 'bank', 'bankruptci',\n",
       "       'base', 'battl', 'beat', 'becam', 'becom', 'began', 'begin',\n",
       "       'behind', 'believ', 'benefit', 'best', 'better', 'biggest', 'bill',\n",
       "       'billion', 'black', 'blair', 'block', 'blog', 'board', 'bodi',\n",
       "       'book', 'boost', 'boss', 'bought', 'brand', 'break', 'bring',\n",
       "       'britain', 'british', 'broadband', 'broadcast', 'brought', 'brown',\n",
       "       'budget', 'build', 'bush', 'busi', 'buy', 'call', 'came', 'camera',\n",
       "       'campaign', 'campbel', 'captain', 'card', 'care', 'career',\n",
       "       'carri', 'case', 'cash', 'categori', 'caus', 'celebr', 'centr',\n",
       "       'central', 'ceremoni', 'certain', 'chairman', 'challeng',\n",
       "       'champion', 'championship', 'chanc', 'chancellor', 'chang',\n",
       "       'channel', 'charg', 'charl', 'chart', 'chelsea', 'chief',\n",
       "       'chief execut', 'children', 'china', 'chines', 'choic', 'christma',\n",
       "       'citi', 'civil', 'claim', 'clark', 'clear', 'close', 'club',\n",
       "       'coach', 'code', 'collect', 'combin', 'come', 'comedi', 'comment',\n",
       "       'commiss', 'commit', 'committe', 'common', 'communic', 'communiti',\n",
       "       'compani', 'compar', 'compet', 'competit', 'complet', 'comput',\n",
       "       'concern', 'confer', 'confid', 'confirm', 'connect', 'conserv',\n",
       "       'consid', 'consol', 'consult', 'consum', 'content', 'contest',\n",
       "       'continu', 'contract', 'control', 'controversi', 'copi', 'corpor',\n",
       "       'cost', 'could', 'council', 'countri', 'coupl', 'cours', 'court',\n",
       "       'creat', 'credit', 'crime', 'crimin', 'critic', 'criticis',\n",
       "       'cross', 'crowd', 'current', 'custom', 'cut', 'damag', 'data',\n",
       "       'date', 'davi', 'david', 'day', 'deal', 'death', 'debat', 'debt',\n",
       "       'debut', 'decemb', 'decid', 'decis', 'declin', 'defeat', 'defenc',\n",
       "       'defend', 'deficit', 'deliv', 'dem', 'demand', 'democrat', 'deni',\n",
       "       'depart', 'describ', 'design', 'despit', 'detail', 'develop',\n",
       "       'devic', 'die', 'differ', 'difficult', 'digit', 'direct',\n",
       "       'director', 'disappoint', 'disast', 'discuss', 'display',\n",
       "       'distribut', 'document', 'dollar', 'domin', 'done', 'doubl',\n",
       "       'doubt', 'download', 'drama', 'draw', 'drive', 'drop', 'drug',\n",
       "       'duti', 'earli', 'earlier', 'earn', 'east', 'econom', 'economi',\n",
       "       'economist', 'educ', 'effect', 'effort', 'eight', 'either',\n",
       "       'elect', 'electron', 'email', 'emerg', 'employ', 'encourag', 'end',\n",
       "       'energi', 'engin', 'england', 'enjoy', 'enough', 'ensur', 'enter',\n",
       "       'entertain', 'estim', 'euro', 'europ', 'european', 'even', 'event',\n",
       "       'ever', 'everi', 'everyon', 'everyth', 'evid', 'exampl', 'exchang',\n",
       "       'execut', 'exist', 'expect', 'experi', 'expert', 'explain',\n",
       "       'export', 'express', 'extra', 'face', 'fact', 'fail', 'fair',\n",
       "       'fall', 'famili', 'fan', 'favour', 'favourit', 'fear', 'featur',\n",
       "       'februari', 'feder', 'feel', 'fell', 'felt', 'festiv', 'field',\n",
       "       'fight', 'figur', 'file', 'film', 'final', 'financ', 'financi',\n",
       "       'find', 'fine', 'finish', 'firm', 'first', 'first time', 'five',\n",
       "       'focus', 'follow', 'footbal', 'forc', 'forecast', 'foreign',\n",
       "       'form', 'former', 'forward', 'found', 'four', 'fourth', 'franc',\n",
       "       'fraud', 'free', 'french', 'friday', 'friend', 'front', 'full',\n",
       "       'fund', 'futur', 'gadget', 'gain', 'game', 'gave', 'general',\n",
       "       'general elect', 'generat', 'german', 'germani', 'get', 'giant',\n",
       "       'give', 'given', 'global', 'go', 'goal', 'gold', 'golden', 'gone',\n",
       "       'good', 'googl', 'gordon', 'gordon brown', 'govern', 'grand',\n",
       "       'grand slam', 'great', 'ground', 'group', 'grow', 'growth', 'half',\n",
       "       'hand', 'handset', 'happen', 'happi', 'hard', 'head', 'health',\n",
       "       'hear', 'held', 'help', 'high', 'higher', 'histori', 'hold',\n",
       "       'hollywood', 'home', 'home secretari', 'honour', 'hope', 'host',\n",
       "       'hour', 'hous', 'howard', 'howev', 'huge', 'human', 'hunt', 'idea',\n",
       "       'illeg', 'imag', 'immedi', 'immigr', 'impact', 'import', 'impress',\n",
       "       'improv', 'includ', 'incom', 'increas', 'independ', 'india',\n",
       "       'indian', 'individu', 'industri', 'inflat', 'inform', 'initi',\n",
       "       'injuri', 'insist', 'instead', 'institut', 'insur', 'interest',\n",
       "       'interest rate', 'intern', 'internet', 'introduc', 'invest',\n",
       "       'investig', 'investor', 'involv', 'iraq', 'ireland', 'irish',\n",
       "       'issu', 'itali', 'jame', 'januari', 'japan', 'job', 'john',\n",
       "       'johnson', 'join', 'jone', 'judg', 'juli', 'june', 'keen', 'keep',\n",
       "       'kelli', 'kennedi', 'kick', 'kill', 'kilroysilk', 'kind', 'king',\n",
       "       'know', 'known', 'labour', 'lack', 'land', 'larg', 'largest',\n",
       "       'last', 'last month', 'last week', 'last year', 'late', 'later',\n",
       "       'latest', 'launch', 'law', 'lawyer', 'lead', 'leader', 'leagu',\n",
       "       'least', 'leav', 'left', 'legal', 'less', 'level', 'liber',\n",
       "       'liber democrat', 'life', 'light', 'like', 'limit', 'line', 'link',\n",
       "       'list', 'listen', 'littl', 'live', 'liverpool', 'local', 'london',\n",
       "       'long', 'longer', 'look', 'lord', 'lose', 'loss', 'lost', 'love',\n",
       "       'lower', 'machin', 'made', 'madrid', 'magazin', 'main', 'major',\n",
       "       'make', 'maker', 'manag', 'manchest', 'manchest unit', 'mani',\n",
       "       'manufactur', 'march', 'mark', 'market', 'martin', 'match',\n",
       "       'matter', 'mean', 'meanwhil', 'measur', 'media', 'meet', 'member',\n",
       "       'messag', 'michael', 'michael howard', 'microsoft', 'might',\n",
       "       'mike', 'million', 'mini', 'minist', 'minut', 'miss', 'mobil',\n",
       "       'mobil phone', 'model', 'moment', 'monday', 'money', 'monitor',\n",
       "       'month', 'move', 'movi', 'much', 'music', 'must', 'name', 'nation',\n",
       "       'near', 'need', 'network', 'never', 'news', 'newspap', 'next',\n",
       "       'next year', 'night', 'nomin', 'north', 'noth', 'novemb', 'number',\n",
       "       'obvious', 'octob', 'offer', 'offic', 'offici', 'often', 'olymp',\n",
       "       'onlin', 'open', 'oper', 'opportun', 'opposit', 'order', 'organis',\n",
       "       'origin', 'oscar', 'other', 'outsid', 'owner', 'paid', 'pair',\n",
       "       'paper', 'parent', 'park', 'parliament', 'part', 'parti',\n",
       "       'particular', 'pass', 'past', 'paul', 'penalti', 'pension',\n",
       "       'peopl', 'perform', 'period', 'person', 'peter', 'phone', 'pick',\n",
       "       'pictur', 'place', 'plan', 'play', 'player', 'pledg', 'point',\n",
       "       'polic', 'polici', 'polit', 'poll', 'poor', 'popular', 'portabl',\n",
       "       'posit', 'possibl', 'post', 'potenti', 'power', 'predict',\n",
       "       'premiership', 'prepar', 'present', 'presid', 'press', 'pressur',\n",
       "       'prevent', 'previous', 'price', 'prime', 'prime minist', 'privat',\n",
       "       'prize', 'probabl', 'problem', 'process', 'produc', 'product',\n",
       "       'profit', 'program', 'programm', 'project', 'promis', 'promot',\n",
       "       'properti', 'propos', 'protect', 'prove', 'provid', 'public',\n",
       "       'publish', 'pull', 'push', 'put', 'qualiti', 'quarter', 'question',\n",
       "       'quick', 'quit', 'race', 'radio', 'rais', 'rang', 'rate', 'rather',\n",
       "       'reach', 'read', 'real', 'realli', 'reason', 'receiv', 'recent',\n",
       "       'record', 'reduc', 'refere', 'reflect', 'reform', 'refus',\n",
       "       'region', 'regul', 'reject', 'relat', 'releas', 'remain', 'replac',\n",
       "       'report', 'repres', 'requir', 'research', 'respect', 'respond',\n",
       "       'respons', 'rest', 'result', 'retail', 'retir', 'return', 'reveal',\n",
       "       'revenu', 'review', 'richard', 'right', 'rise', 'risk', 'rival',\n",
       "       'road', 'robert', 'robinson', 'rock', 'roddick', 'role', 'rose',\n",
       "       'round', 'rugbi', 'rule', 'run', 'russia', 'russian', 'said would',\n",
       "       'sale', 'saturday', 'save', 'say', 'scheme', 'school', 'score',\n",
       "       'scotland', 'scottish', 'screen', 'search', 'season', 'seat',\n",
       "       'second', 'secretari', 'sector', 'secur', 'see', 'seed', 'seek',\n",
       "       'seem', 'seen', 'sell', 'send', 'senior', 'sent', 'separ',\n",
       "       'septemb', 'seri', 'serious', 'serv', 'servic', 'seven', 'sever',\n",
       "       'share', 'sharehold', 'short', 'shot', 'show', 'shown', 'side',\n",
       "       'sign', 'signific', 'similar', 'simpli', 'sinc', 'singer', 'singl',\n",
       "       'site', 'situat', 'slam', 'slow', 'small', 'smith', 'social',\n",
       "       'societi', 'softwar', 'sold', 'someth', 'song', 'soni', 'soon',\n",
       "       'sound', 'sourc', 'south', 'speak', 'special', 'specul', 'speech',\n",
       "       'speed', 'spend', 'spent', 'spokesman', 'spokesman said', 'sport',\n",
       "       'spot', 'squad', 'staff', 'stage', 'stand', 'standard', 'star',\n",
       "       'start', 'state', 'statement', 'station', 'stay', 'step', 'still',\n",
       "       'stock', 'stop', 'store', 'stori', 'street', 'strike', 'strong',\n",
       "       'struggl', 'student', 'studi', 'studio', 'success', 'suffer',\n",
       "       'suggest', 'summer', 'sunday', 'suppli', 'support', 'sure',\n",
       "       'surpris', 'survey', 'suspect', 'system', 'tackl', 'take', 'taken',\n",
       "       'talk', 'target', 'tax', 'team', 'technolog', 'televis', 'tell',\n",
       "       'term', 'terror', 'test', 'thank', 'theatr', 'thing', 'think',\n",
       "       'third', 'though', 'thought', 'thousand', 'threat', 'three',\n",
       "       'thursday', 'time', 'titl', 'today', 'togeth', 'told', 'told news',\n",
       "       'toni', 'toni blair', 'took', 'tool', 'tori', 'total', 'tough',\n",
       "       'tour', 'toward', 'track', 'trade', 'train', 'travel', 'tri',\n",
       "       'trial', 'troubl', 'trust', 'tsunami', 'tuesday', 'turn', 'ukip',\n",
       "       'understand', 'union', 'unit', 'univers', 'unlik', 'unveil', 'urg',\n",
       "       'use', 'user', 'valu', 'version', 'victim', 'victori', 'video',\n",
       "       'view', 'viewer', 'virus', 'visit', 'visitor', 'vote', 'voter',\n",
       "       'wage', 'wait', 'wale', 'want', 'warn', 'watch', 'websit',\n",
       "       'wednesday', 'week', 'weekend', 'well', 'went', 'west', 'whether',\n",
       "       'white', 'whole', 'whose', 'wide', 'william', 'win', 'window',\n",
       "       'winner', 'within', 'without', 'women', 'word', 'work', 'worker',\n",
       "       'world', 'worri', 'worth', 'would', 'write', 'written', 'wrong',\n",
       "       'year', 'yearold', 'york', 'young', 'yuko', 'zealand'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97259fd4-6742-4033-a181-3471ec31b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components = 5, random_state = 5510)\n",
    "\n",
    "W1 = clf.fit_transform(tfidf_vectors)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce39a4d2-2940-43ac-b370-f98357592f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00698201, 0.02300131, 0.        , ..., 0.00095059, 0.20736017,\n",
       "        0.        ],\n",
       "       [0.01921616, 0.04364995, 0.        , ..., 0.06303685, 0.        ,\n",
       "        0.09709774],\n",
       "       [0.00042829, 0.01323801, 0.16615939, ..., 0.0587904 , 0.        ,\n",
       "        0.00226522],\n",
       "       [0.01967389, 0.02638005, 0.        , ..., 0.03369955, 0.        ,\n",
       "        0.00080749],\n",
       "       [0.03138191, 0.07921708, 0.        , ..., 0.01773869, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1\n",
    "# video: 16:35 - purpose of H matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "173a8882-b987-43a2-b44f-8d8809affcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07292985, 0.00626263, 0.00784127, 0.00979723, 0.00853967],\n",
       "       [0.14137859, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.07379133, 0.00861941, 0.00063403, 0.04671532, 0.02646402],\n",
       "       ...,\n",
       "       [0.12420799, 0.01191492, 0.00250395, 0.        , 0.        ],\n",
       "       [0.01965571, 0.        , 0.01639135, 0.        , 0.24374781],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.15544705]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1\n",
    "# video: 17:00 - purpose of W matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f28f7be-72af-4b4e-8ac3-26d9b89561a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 15\n",
    "\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "topic_words = ([top_words(t) for t in H1])\n",
    "topics = [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "624d68a0-6a23-4032-8304-80fc52d9e493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['market firm compani sale growth year bank share profit economi price rate econom china trade',\n",
       " 'game play england player match champion team injuri wale side ireland first club coach world',\n",
       " 'film award best star actor nomin oscar actress director festiv movi music includ comedi year',\n",
       " 'labour elect blair parti tori brown minist govern would prime prime minist howard lord campaign chancellor',\n",
       " 'mobil phone peopl technolog music use servic user comput softwar digit network broadband game microsoft']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09cf372e-8863-464e-9ec7-4ec433135d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['Topic' + str(i) for i in range(clf.n_components)]\n",
    "docnames = ['Doc' + str(i) for i in range(len(train_df.Text))]\n",
    "doc_topic_df = pd.DataFrame(np.round(W1, 2), columns = colnames, index = docnames)\n",
    "significant_topic = np.argmax(doc_topic_df.values, axis = 1)\n",
    "doc_topic_df['dominant_topic'] = significant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23f9d3f6-bd40-4ae7-8ea2-2e847668ba49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc8</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic0  Topic1  Topic2  Topic3  Topic4  dominant_topic\n",
       "Doc0    0.07    0.01    0.01    0.01    0.01               0\n",
       "Doc1    0.14    0.00    0.00    0.00    0.00               0\n",
       "Doc2    0.07    0.01    0.00    0.05    0.03               0\n",
       "Doc3    0.00    0.00    0.00    0.00    0.27               4\n",
       "Doc4    0.10    0.01    0.03    0.00    0.01               0\n",
       "Doc5    0.00    0.06    0.00    0.07    0.02               3\n",
       "Doc6    0.00    0.15    0.00    0.00    0.00               1\n",
       "Doc7    0.00    0.00    0.19    0.03    0.00               2\n",
       "Doc8    0.13    0.00    0.00    0.00    0.00               0\n",
       "Doc9    0.02    0.01    0.13    0.01    0.00               2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dc9e7af-d460-4ff3-a830-e06457b3a85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1582</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>651</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1797</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2034</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1866</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text       Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...       business\n",
       "1        154  german business confidence slides german busin...       business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...       business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...           tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...       business\n",
       "5       1582  howard  truanted to play snooker  conservative...       politics\n",
       "6        651  wales silent on grand slam talk rhys williams ...          sport\n",
       "7       1797  french honour for director parker british film...  entertainment\n",
       "8       2034  car giant hit by mercedes slump a slump in pro...       business\n",
       "9       1866  fockers fuel festive film chart comedy meet th...  entertainment"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e59b2b-53ff-4912-ada7-7bf2d3081642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# Extract the topic-term and document-topic matrices\n",
    "nmf_topics = nmf_model.components_  # shape: (n_topics, n_features)\n",
    "doc_topic_dist = nmf_model.transform(tfidf_matrix)  # shape: (n_docs, n_topics)\n",
    "\n",
    "# Create the term frequency matrix from the vectorizer\n",
    "term_freq = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Create the data dictionary required by PyLDAvis\n",
    "data = {\n",
    "    'topic_term_dists': nmf_topics / nmf_topics.sum(axis=1)[:, None],  # Normalize topic-term matrix\n",
    "    'doc_topic_dists': doc_topic_dist / doc_topic_dist.sum(axis=1)[:, None],  # Normalize document-topic matrix\n",
    "    'doc_lengths': np.sum(tfidf_matrix, axis=1).A1,  # Document lengths\n",
    "    'vocab': vectorizer.get_feature_names_out(),  # List of all terms in the vocabulary\n",
    "    'term_frequency': term_freq  # Term frequencies\n",
    "}\n",
    "\n",
    "# Create a PyLDAvis prepared object\n",
    "vis_data = pyLDAvis.prepare(**data)\n",
    "\n",
    "# Visualize the NMF topics\n",
    "pyLDAvis.display(vis_data)  # Opens an interactive display\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
